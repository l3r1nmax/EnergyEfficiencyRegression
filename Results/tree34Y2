[Command: python -u '/Users/lerin/Documents/Uni/TUWien/Machine Learning/Exercise 2/plot_tree_regression.py']
===================================================================
Decision Tree Regression
===================================================================

A 1D regression with decision tree.

The :ref:`decision trees <tree>` is
used to fit a sine curve with addition noisy observation. As a result, it
learns local linear regressions approximating the sine curve.

We can see that if the maximum depth of the tree (controlled by the
`max_depth` parameter) is set too high, the decision trees learn too fine
details of the training data and learn from the noise, i.e. they overfit.

DecisionTree regression 1 score: 0.802
 Mean squared error: 18.2146
 Time: 0.000075
DecisionTree regression 2 score: 0.817
 Mean squared error: 16.8217
 Time: 0.000053
DecisionTree regression 3 score: 0.821
 Mean squared error: 16.4314
 Time: 0.000049
DecisionTree regression 4 score: 0.821
 Mean squared error: 16.4314
 Time: 0.000044
DecisionTree regression 5 score: 0.821
 Mean squared error: 16.4314
 Time: 0.000043
/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/matplotlib/collections.py:548: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  if self._edgecolors == 'face':
